{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 처리\n",
    "\n",
    "- https://mangastorytelling.tistory.com/entry/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC-%EC%9E%85%EB%AC%B8-1106-%EB%84%A4%EC%9D%B4%EB%B2%84-%EC%98%81%ED%99%94-%EB%A6%AC%EB%B7%B0-%EA%B0%90%EC%84%B1-%EB%B6%84%EB%A5%98%ED%95%98%EA%B8%B0Naver-Movie-Review-Sentiment-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_table('ratings.txt')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        200000 non-null  int64 \n",
      " 1   document  199992 non-null  object\n",
      " 2   label     200000 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 199992 entries, 0 to 199999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        199992 non-null  int64 \n",
      " 1   document  199992 non-null  object\n",
      " 2   label     199992 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# 데이터 중복 제거\n",
    "data = data.dropna(axis=0)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                  어릴때보고 지금다시봐도 재밌어요ㅋㅋ\n",
       "1    디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...\n",
       "2                 폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.\n",
       "3    와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...\n",
       "4                          안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.\n",
       "Name: document, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글과 공백을 제외하고 모두 제거\n",
    "data['document'] = data['document'].str.replace('[^가-힣\\s]','')\n",
    "data['document'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 및 데이터 토큰화\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "result = []\n",
    "okt = Okt()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "stopwords\n",
    "\n",
    "for sentence in data['document']:\n",
    "    text = okt.morphs(sentence,stem=True) # 텍스트를 형태소 단위로 나눔\n",
    "    # stem : 각 단어에서 어간을 추출\n",
    "    text = [i for i in text if i not in stopwords]\n",
    "    result += [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 56705\n",
      "등장 빈도가 2번 이하인 희귀 단어의 수: 32787\n",
      "단어 집합에서 희귀 단어의 비율: 57.820298033683095\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.61392158833389\n"
     ]
    }
   ],
   "source": [
    "# 등장 빈도수가 3회 미만인 단어들의 분포 확인\n",
    "\n",
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 23919\n"
     ]
    }
   ],
   "source": [
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(result)\n",
    "# x = tokenizer.texts_to_sequences(result) # 단어 -> 숫자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "import json\n",
    "json = json.dumps(tokenizer.word_index)\n",
    "f3 = open(\"wordIndex.json\", \"w\")\n",
    "f3.write(json)\n",
    "f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = list(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_idx = [i for i in range(len(x)) if len(x[i])==0] # 길이가 0인 문자열 제거\n",
    "len(drop_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in x if len(i)!=0]\n",
    "y = [y[i] for i in range(len(y)) if i not in drop_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199613 199613\n"
     ]
    }
   ],
   "source": [
    "print(len(x),len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.84365246752466\n"
     ]
    }
   ],
   "source": [
    "cnt = 0 \n",
    "for i in x:\n",
    "    if len(i) <= 30:\n",
    "        cnt += 1\n",
    "\n",
    "print((cnt/len(x))*100) # 길이가 30이하인 리스트의가 전체 리스트의 92.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x = pad_sequences(x,maxlen = 30) # max 길이를 30으로 조정\n",
    "# 길이가 20이면 그만큼 0으로 채워짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM,Bidirectional\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199613, 30)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 검증\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(x,np.array(y),test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159690, 30)\n",
      "(159690,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127752, 30) (127752,)\n",
      "(31938, 30) (31938,)\n"
     ]
    }
   ],
   "source": [
    "xtrain, val_data, ytrain, val_test = train_test_split(xtrain,ytrain,test_size=0.2, random_state=42)\n",
    "print(xtrain.shape,ytrain.shape)\n",
    "print(val_data.shape,val_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2130/2130 [==============================] - ETA: 0s - loss: 0.3972 - acc: 0.8200\n",
      "Epoch 1: val_acc improved from -inf to 0.83878, saving model to best_model.h5\n",
      "2130/2130 [==============================] - 208s 95ms/step - loss: 0.3972 - acc: 0.8200 - val_loss: 0.3651 - val_acc: 0.8388\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amysm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2130/2130 [==============================] - ETA: 0s - loss: 0.3321 - acc: 0.8585\n",
      "Epoch 2: val_acc improved from 0.83878 to 0.84749, saving model to best_model.h5\n",
      "2130/2130 [==============================] - 215s 101ms/step - loss: 0.3321 - acc: 0.8585 - val_loss: 0.3468 - val_acc: 0.8475\n",
      "Epoch 3/15\n",
      "2130/2130 [==============================] - ETA: 0s - loss: 0.3020 - acc: 0.8736\n",
      "Epoch 3: val_acc improved from 0.84749 to 0.85835, saving model to best_model.h5\n",
      "2130/2130 [==============================] - 198s 93ms/step - loss: 0.3020 - acc: 0.8736 - val_loss: 0.3292 - val_acc: 0.8584\n",
      "Epoch 4/15\n",
      "2130/2130 [==============================] - ETA: 0s - loss: 0.2775 - acc: 0.8870\n",
      "Epoch 4: val_acc improved from 0.85835 to 0.86148, saving model to best_model.h5\n",
      "2130/2130 [==============================] - 190s 89ms/step - loss: 0.2775 - acc: 0.8870 - val_loss: 0.3277 - val_acc: 0.8615\n",
      "Epoch 5/15\n",
      "2130/2130 [==============================] - ETA: 0s - loss: 0.2566 - acc: 0.8981\n",
      "Epoch 5: val_acc did not improve from 0.86148\n",
      "2130/2130 [==============================] - 193s 91ms/step - loss: 0.2566 - acc: 0.8981 - val_loss: 0.3335 - val_acc: 0.8574\n",
      "Epoch 6/15\n",
      "2130/2130 [==============================] - ETA: 0s - loss: 0.2363 - acc: 0.9082\n",
      "Epoch 6: val_acc did not improve from 0.86148\n",
      "2130/2130 [==============================] - 194s 91ms/step - loss: 0.2363 - acc: 0.9082 - val_loss: 0.3347 - val_acc: 0.8614\n",
      "Epoch 7/15\n",
      "2130/2130 [==============================] - ETA: 0s - loss: 0.2177 - acc: 0.9166\n",
      "Epoch 7: val_acc did not improve from 0.86148\n",
      "2130/2130 [==============================] - 183s 86ms/step - loss: 0.2177 - acc: 0.9166 - val_loss: 0.3441 - val_acc: 0.8556\n",
      "Epoch 8/15\n",
      "2129/2130 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9248\n",
      "Epoch 8: val_acc did not improve from 0.86148\n",
      "2130/2130 [==============================] - 188s 88ms/step - loss: 0.2009 - acc: 0.9248 - val_loss: 0.3570 - val_acc: 0.8586\n",
      "Epoch 8: early stopping\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(xtrain, ytrain, epochs=15, callbacks=[es, mc], batch_size=60, validation_data=(val_data,val_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1248/1248 [==============================] - 23s 19ms/step - loss: 0.3520 - acc: 0.8579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35202962160110474, 0.8579264879226685]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_result = model.evaluate(xtest, ytest)\n",
    "evaluate_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 정확도 측정 -> 모델을 불러올 때 유니코드 에러 발생\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "loaded_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "import json\n",
    "\n",
    "# Open the JSON file\n",
    "with open('for_korean.json', 'r') as f:\n",
    "    # Read the JSON data and store in word_index\n",
    "    word_index = json.load(f)\n",
    "\n",
    "# Now you can use the word_index variable, which contains the data from the JSON file\n",
    "tokenizer.word_index = word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = 30) # 길이 맞춤\n",
    "    score = float(loaded_model.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "      return (\"{:.2f}% 확률로 긍정 리뷰입니다.\".format(score * 100))\n",
    "    else:\n",
    "      return (\"{:.2f}% 확률로 부정 리뷰입니다.\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'75.49% 확률로 부정 리뷰입니다.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_predict('스트레스로 인해 매우 불안해하고 여유가 없음')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'79.41% 확률로 긍정 리뷰입니다.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_predict('불안함이 완전히 없어지지는 않았지만 마음이 편안해진 것이 보임')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
